# Domain 4.3 – Vulnerability Management

> **Vulnerability Life Cycle**  
> **Identification → Analysis → Response → Validation → Reporting**

---

## Identification Methods

### SAST (Static Application Security Testing)
- Detects: **buffer overflows, database injections**  
- Cannot detect: **auth security, insecure cryptography**  
- Must verify findings → **false positives** common  

### Dynamic Analysis (Fuzzing)
- Send random input to app → watch for **crash, errors, exceptions**  
- Types: fault-injection, robustness, syntax, negative testing  
- **Fuzzing engines & frameworks**:  
  - Platform or language specific  
  - **Resource heavy** (CPU, time) → many use probability tests  
  - Example: **CERT Basic Fuzzing Framework (BFF)**  

### Package Monitoring
- Confirm **legitimacy** of packages:  
  - Trusted source  
  - No added malware / vulnerabilities  
  - Contents verified  

### Threat Intelligence
#### OSINT (Open-source Intelligence)
- Public sources: Internet, social media, govt data, reports  

#### Proprietary / Third-Party
- Purchased intelligence services  
- Correlate threat data across sources  
- Automated prevention workflows  

#### Information-Sharing Orgs
- Public intel (sometimes classified)  
- Private intel (industry groups, e.g. **Cyber Threat Alliance**)  
  - Members share validated, scored threat data  

#### Dark Web Intelligence
- Monitor hacker forums/markets:  
  - Tools/techniques, credit card sales, stolen acc/pw  
  - Good for checking if your org is being targeted  

### Penetration Testing
- Simulates **real attack** (unlike vuln scan which only detects)  
- Often compliance mandated  

**Cautions**  
- May cause **DoS / data loss** if misused  
- Exploits: brute-force, social engineering, injection, buffer overflow  

#### Rules of Engagement
- Defines **scope & purpose**:  
  - Testing type (internal, external, physical, after hours)  
  - Rules (IP ranges, handling sensitive info, in/out-of-scope devices)  

#### Process
- **Initial exploitation** → entry point  
- **Lateral movement** → jump between systems  
- **Persistence** → backdoors, default pw changes, user creation  
- **Pivot point** → compromised system as proxy  

#### Responsible Disclosure
- Report vulnerabilities → vendor fixes  
- **Bug bounty programs** incentivize disclosure  

**Cheatsheet – Identification**

| **Method** | **Key Point** | **Exam Trap** |
|------------|---------------|---------------|
| SAST | Static code flaws | False positives |
| Fuzzing | Dynamic crash tests | Resource heavy |
| Package Monitoring | Verify source & integrity | Malware injection |
| OSINT | Public info | Social media/government |
| Proprietary Intel | Purchased services | Automated workflows |
| Info-Sharing | CTA, member orgs | Validated intel |
| Dark Web Intel | Stolen data markets | Acc/pw sales |
| Pentest | Exploitation attempt | Can cause DoS |

---

## Analysis

### False Positives vs False Negatives
- **False positive** → flagged vuln doesn’t exist  
- **False negative** → vuln exists but wasn’t detected  
- Keep tools updated (signatures, DB).  

### Prioritization
- **Severity ranking**: High > Medium > Low  
- **CVSS** → score 0–10 (v2 vs v3.x differences)  
- **CVE** → known vulnerabilities (e.g. CVE-2020-1889)  
  - Apps, web apps, network misconfigs  

### Exposure Factor
- % impact: e.g. DDoS = 50% service loss, buffer overflow = 100% loss  

### Environmental Variables
- Internal vs external, public vs test environment  
- Patching frequency depends on user type, revenue apps  

### Org Impact & Risk Tolerance
- Org type influences severity  
- Risk tolerance defines **patch timing**  
- Testing introduces exposure window  

**Cheatsheet – Analysis**

| **Concept** | **Definition** | **Exam Trap** |
|-------------|----------------|---------------|
| False Positive | Vuln flagged but not real | Over-reporting |
| False Negative | Vuln exists but missed | Blind spot |
| CVSS | Quantitative 0–10 score | Version v2 vs v3 |
| CVE | Specific vuln ID | Applies across scans |
| Exposure Factor | % service loss | 50% vs 100% |
| Risk Tolerance | Patch timing acceptable | Depends on severity |

---

## Vulnerability Response & Remediation

### Patching
- **Most common mitigation**  
- Scheduled (monthly, quarterly)  
- Unscheduled (zero-day, urgent)  

### Insurance
- Cyber coverage may include: lost revenue, data recovery, phishing losses, privacy lawsuits  
- Not comprehensive; ransomware coverage growing  

### Segmentation
- Limit exploit scope  
- **Air gap** → if can’t patch, disconnect from world  
- Internal **NGFWs**: block traffic, detect malicious flows  

### Physical vs Logical Segmentation
- **Physical** → separate devices/infrastructure  
- **Logical (VLANs)** → segmented on same device, requires router (L3) to communicate  

### Compensating Controls
- Alternative measures when patch unavailable:  
  - Disable service  
  - Revoke access  
  - Limit external use  
  - Modify internal controls, software firewalls  

**Cheatsheet – Response**

| **Technique** | **Purpose** | **Exam Trap** |
|---------------|-------------|---------------|
| Patching | Fix vuln directly | Zero-day urgent |
| Insurance | Offset financial loss | Limited coverage |
| Segmentation | Limit attack scope | VLANs need L3 device |
| Air Gap | Isolation when no patch | ICS/SCADA |
| Comp Controls | Temporary protection | Disable service |

---

## Validation of Remediation

### Exceptions & Exemptions
- Balance between service availability vs patching risk  
- Some vulns less severe → delayed fixes  

### Validation Process
- Confirm patch effectiveness  
- Verify all vulnerable systems patched  
- **Rescan** after patch  
- **Audit & verification**  

---

## Reporting
- Continuous checks needed → vulns always emerging  
- **Automation essential** (manual too time-consuming)  
- Reporting includes:  
  - # identified vulns  
  - Patched vs unpatched systems  
  - New threat notifications  
  - Errors, exceptions, exemptions  

---

# Exam Triggers Quick Recall – Vulnerability Management (4.3)

- **Lifecycle** → Identification → Analysis → Response → Validation → Reporting  
- **SAST** = static flaws, false positives  
- **Fuzzing** = dynamic random input, crashes  
- **Package monitoring** = validate sources  
- **OSINT** = public info  
- **Proprietary intel** = purchased services  
- **Info-sharing orgs** = CTA, validated intel  
- **Dark web intel** = stolen creds, CC sales  
- **Pentest** = exploit attempt, rules of engagement, pivot/lateral move/persistence  
- **False positive** = vuln doesn’t exist  
- **False negative** = vuln exists but missed  
- **CVSS** = numeric score  
- **CVE** = vuln identifier  
- **Exposure factor** = % service loss  
- **Segmentation** = VLANs vs physical  
- **Air gap** = ICS/SCADA, unpatchable system  
- **Compensating controls** = temp measures  
- **Validation** = rescans, audits  
- **Reporting** = automation, continuous monitoring
